{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Naive Bayes from scratch](https://philippmuens.com/naive-bayes-from-scratch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem Example\n",
    "#### Problem\n",
    "- What's the probability that it'll be sunny throughout the day given that we saw clouds? Given\n",
    "    + The chance to see clouds is: 60%\n",
    "    + Only 7 out of 30 days can be considered sunny\n",
    "    + 50% of the sunny days started out with clouds\n",
    "\n",
    "#### Solution\n",
    "- P(Cloud) = 0.6\n",
    "- P(Sun) = 0.23\n",
    "- P(Cloud | Sun) = 0.5\n",
    "- Probability that it'll be sunny throughout the day given that we saw clouds:\n",
    "    + P(Sun | Cloud) = $\\frac{P(Cloud\\ |\\ Sun)\\ *\\ P(Sun)}{P(Cloud)} = \\frac{0.5*0.23}{0.6} = 19.17 \\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem and spammy words\n",
    "## 1. Detect if an message given a word is a spam mail?\n",
    "- Assume that we're working with the word \"prize\" and want to figure out what the probability is that a given message which contains the word \"prize\" is spam\n",
    "- Formulating this problem via Bayes Theorem: $P(Spam\\ |\\ Prize)\\ =\\ \\frac{P(Prize\\ |\\ Spam)\\ *\\ P(Spam)}{P(Prize)}$\n",
    "    - $P(Spam\\ |\\ Prize)$: probability of a message being spam given that we see the word \"Prize\" in it\n",
    "    - $P(Prize\\ |\\ Spam)$: the probability of finding the word \"Prize\" in our already seen spam messages\n",
    "    - $P(Spam)$: the probability of the message being spam\n",
    "    - $P(Prize)$: probability of finding the word \"Prize\" in any of our already seen messages\n",
    "\n",
    "- Expanded Bayes Theorem: $P(Spam\\ |\\ Prize)\\ =\\ \\frac{P(Prize\\ |\\ Spam)\\ *\\ P(Spam)}{P(Prize\\ ∣\\ Spam)\\ *\\ P(Spam)\\ +\\ P(Prize\\ ∣\\ Ham)\\ *\\ P(Ham)}$\n",
    "- Initially we dont know the the probability of the message being spam or ham: $P(Spam)\\ =\\ P(Ham)\\ =\\ 0.5$\n",
    "\n",
    "$$\\begin{equation}\n",
    "    \\begin{split}\n",
    "        P(Spam\\ |\\ Prize)\\ &=\\ \\frac{P(Prize\\ |\\ Spam)\\ *\\ 0.5}{P(Prize\\ ∣\\ Spam)\\ *\\ 0.5\\ +\\ P(Prize\\ ∣\\ Ham)\\ *\\ 0.5} \\\\\n",
    "            & = \\frac{P(Prize\\ |\\ Spam)}{P(Prize\\ ∣\\ Spam)\\ +\\ P(Prize\\ ∣\\ Ham)}\n",
    "    \\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "## 2. Scaling it up to whole sentences\n",
    "- A naive approach = insert the whole message as the conditional probability part\n",
    "    - A sentence = a succession of single words\n",
    "    + Apply single-word model to every word we come across in our message\n",
    "    + Computing the overall probability for the whole message = multiplication of all those individual probabilities\n",
    "\n",
    "- Example:\n",
    "    $$\\begin{split}\n",
    "        P(Message\\ |\\ Spam)\\ &=\\ P(word_1, word_2, ..., word_n\\ |\\ Spam) \\\\\n",
    "                            &=\\ P(word_1\\ |\\ Spam)\\ *\\ P(word_2\\ |\\ Spam)\\ *\\ ...\\ *\\ P(word_n\\ |\\ Spam)\n",
    "    \\end{split}$$\n",
    "\n",
    "## 3. Difficulties\n",
    "- Difficulty 1: Vanishing Probility in computation\n",
    "    + Multiply a lot of small probabilities\n",
    "    + Computer architectures are only capable to deal with a certain amount of precision  \n",
    "        $\\to$ \"arithmetic underflow\": eventually turn our number into a 0\n",
    "    - Trick:\n",
    "        + $log(a\\ *\\ b) = log(a)\\ +\\ log(b)$\n",
    "        + $exp(log(a\\ *\\ b)) = a\\ *\\ b$\n",
    "\n",
    "- Difficulty 2: Vanishing Probility in unseen words\n",
    "    + If we've never seen the word W in a spam message, then P(W ∣ Spam) = 0\n",
    "    - Trick: Introduce factor k\n",
    "        + $P(W\\ |\\ S)\\ =\\ \\frac{k\\ +\\ \\text{spam that containing W}}{(2\\ *\\ k)\\ +\\ \\text{total spam}}$\n",
    "        + Example: 100 spam examples but found the word W 0 times\n",
    "            + k = 0: No factor\n",
    "                + $P(W\\ |\\ S)\\ =\\ \\frac{0}{100}\\ = 0$\n",
    "            + k = 1\n",
    "                + $P(W\\ |\\ S)\\ =\\ \\frac{k\\ +\\ 0}{(2*k)\\ +\\ 100}\\ = 0.001$\n",
    "\n",
    "## 4. Implementation\n",
    "- We need to \"train\" our Naive Bayes classifier with training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [[ ! -d \"data/enron1\" ]]; then\n",
    "    wget -nc \\\n",
    "        -P data \\\n",
    "        http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz 2> /dev/null\n",
    "    tar -xzf data/enron1.tar.gz -C data 2> /dev/null\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n",
      "spam\n",
      "Summary.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls data/enron1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer.ham.txt\n",
      "0002.1999-12-13.farmer.ham.txt\n",
      "0003.1999-12-14.farmer.ham.txt\n",
      "0004.1999-12-14.farmer.ham.txt\n",
      "0005.1999-12-14.farmer.ham.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls data/enron1/ham | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0006.2003-12-18.GP.spam.txt\n",
      "0008.2003-12-18.GP.spam.txt\n",
      "0017.2003-12-18.GP.spam.txt\n",
      "0018.2003-12-18.GP.spam.txt\n",
      "0026.2003-12-18.GP.spam.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls data/enron1/spam | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from typing import List\n",
    "\n",
    "\n",
    "spam_message_paths: List[str] = glob('data/enron1/spam/*.txt')\n",
    "ham_message_paths: List[str] = glob('data/enron1/ham/*.txt')\n",
    "message_paths: List[str] = spam_message_paths + ham_message_paths\n",
    "\n",
    "class Message():\n",
    "    def __init__(self, text: str, is_spam: bool) -> None:\n",
    "        self.text: str = text\n",
    "        self.is_spam: bool = is_spam\n",
    "\n",
    "messages: List[Message] = []\n",
    "for path in message_paths:\n",
    "    with open(path, errors='ignore') as file:\n",
    "        is_spam: bool = True if 'spam' in path else False\n",
    "        text: str = ''\n",
    "        for line in file.readlines():\n",
    "            text += line.replace('Subject:', '').strip()\n",
    "            text += ' '\n",
    "        messages.append(\n",
    "            Message(text, is_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam messages: 1500\n",
      "---- Example spam ----\n",
      "we sell regalis for an affordable price hi , regalis , also known as superviagra or cialis - half a pill lasts all weekend - has less sideeffects - has higher success rate now you can buy regalis , for over 70 % cheaper than the equivilent brand for sale in us we ship world wide , and no prescription is required ! ! even if you ' re not impotent , regalis will increase size , pleasure and power ! try it today you wont regret ! get it here : http : / / koolrx . com / sup / best regards , jeremy stones no thanks : http : / / koolrx . com / rm . html \n"
     ]
    }
   ],
   "source": [
    "spam_messages = 0\n",
    "for message in messages:\n",
    "    if message.is_spam == True: spam_messages += 1\n",
    "print(\"Number of spam messages: {}\".format(spam_messages))\n",
    "\n",
    "print('''---- Example spam ----\n",
    "{}'''.format(messages[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ham messages: 1500\n",
      "---- Example ham ----\n",
      "re : killing ena to ena deals in sitara jay , if a deal is killed it poses a problem for us in unify if there are any paths associated with the deal ; therefore , we request the deals be zeroed out . call me if this is a problem . also , we would appreciate further details on why these deals are being killed . in addition , i have copied rita and mark from volume management for their input . regards , tammy x 35375 - - - - - original message - - - - - from : pena , matt sent : thursday , december 13 , 2001 3 : 39 pm to : krishnaswamy , jayant ; pinion , richard ; jaquet , tammy cc : severson , russ ; truong , dat ; aybar , luis ; ma , felicia subject : re : killing ena to ena deals in sitara thanks jay ! tammy / richard : you may want to let the schedulers know , although they may already . - - - - - original message - - - - - from : krishnaswamy , jayant sent : thursday , december 13 , 2001 3 : 38 pm to : pinion , richard ; jaquet , tammy cc : severson , russ ; pena , matt ; truong , dat ; aybar , luis ; ma , felicia subject : killing ena to ena deals in sitara richars / tammy : we will be killing about 2000 deals in sitara tonight . whenever a deal is touched in sitara , it will bridge over to unify . these are desk 2 desk deals , and should have minimal impact on you . \n"
     ]
    }
   ],
   "source": [
    "ham_messages = 0\n",
    "for message in messages:\n",
    "    if message.is_spam == False: ham_messages += 1\n",
    "print(\"Number of ham messages: {}\".format(spam_messages))\n",
    "\n",
    "print('''---- Example ham ----\n",
    "{}'''.format(messages[1505].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from random import shuffle\n",
    "\n",
    "def train_test_split(messages: List[Message], pct=0.8) -> Tuple[List[Message], List[Message]]:\n",
    "    shuffle(messages)\n",
    "    num_train = int(round(len(messages) * pct, 0))\n",
    "    return (messages[:num_train], messages[num_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4396\n",
      "776\n"
     ]
    }
   ],
   "source": [
    "train: List[Message]\n",
    "test: List[Message]\n",
    "\n",
    "train, test = train_test_split(messages, 0.85)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model\n",
    "#### Tokenize\n",
    "- Input: a raw message\n",
    "- Output: a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text: str) -> set():\n",
    "    words = []\n",
    "    for word in re.findall(r'[A-Za-z0-9\\']+', text):\n",
    "        words.append(word.lower())\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'if', 'is', 'so', 'text', 'this', 'tokenize'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert tokenize(\n",
    "    'Is this a text? If so, Tokenize this text!...') == \\\n",
    "    {'is', 'this', 'a', 'text', 'if', 'so', 'tokenize'}\n",
    "\n",
    "tokenize(\n",
    "    'Is this a text? If so, Tokenize this text!...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "from math import log, exp\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, k=1) -> None:\n",
    "        self._k: int = k\n",
    "        self._num_spam_messages: int = 0\n",
    "        self._num_ham_messages: int = 0\n",
    "        self._num_word_in_spam: Dict[int] = defaultdict(int)\n",
    "        self._num_word_in_ham: Dict[int] = defaultdict(int)\n",
    "        self._spam_words: Set[str] = set()\n",
    "        self._ham_words: Set[str] = set()\n",
    "        self._words: Set[str] = set()\n",
    "\n",
    "\n",
    "    def train(self, messages: List[Message]) -> None:\n",
    "        for msg in messages:\n",
    "            tokens: Set[str] = tokenize(msg.text)\n",
    "            self._words.update(tokens)\n",
    "            if msg.is_spam:\n",
    "                self._num_spam_messages += 1\n",
    "                self._spam_words.update(tokens)\n",
    "                for token in tokens:\n",
    "                    self._num_word_in_spam[token] += 1\n",
    "            else:\n",
    "                self._num_ham_messages += 1\n",
    "                self._ham_words.update(tokens)\n",
    "                for token in tokens:\n",
    "                    self._num_word_in_ham[token] += 1\n",
    "\n",
    "\n",
    "    def _p_word_spam(self, word: str) -> float:\n",
    "        return (self._k + self._num_word_in_spam[word]) / ((2 * self._k) + self._num_spam_messages)\n",
    "\n",
    "\n",
    "    def _p_word_ham(self, word: str) -> float:\n",
    "        return (self._k + self._num_word_in_ham[word]) / ((2 * self._k) + self._num_ham_messages)\n",
    "\n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_words: Set[str] = tokenize(text)\n",
    "        log_p_spam: float = 0.0\n",
    "        log_p_ham: float = 0.0\n",
    "\n",
    "        for word in self._words:\n",
    "            p_spam: float = self._p_word_spam(word)\n",
    "            p_ham: float = self._p_word_ham(word)\n",
    "            if word in text_words:\n",
    "                log_p_spam += log(p_spam)\n",
    "                log_p_ham += log(p_ham)\n",
    "            else:\n",
    "                log_p_spam += log(1 - p_spam)\n",
    "                log_p_ham += log(1 - p_ham)\n",
    "\n",
    "        p_if_spam: float = exp(log_p_spam)\n",
    "        p_if_ham: float = exp(log_p_ham)\n",
    "        \n",
    "        try:\n",
    "            return p_if_spam / (p_if_spam + p_if_ham)\n",
    "        except:\n",
    "            return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb: NaiveBayes = NaiveBayes()\n",
    "nb.train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100/776\n",
      "Progress: 200/776\n",
      "Progress: 300/776\n",
      "Progress: 400/776\n",
      "Progress: 500/776\n",
      "Progress: 600/776\n",
      "Progress: 700/776\n",
      "Progress: 776/776\n"
     ]
    }
   ],
   "source": [
    "true_positives: int = 0\n",
    "true_negatives: int = 0\n",
    "\n",
    "false_negatives: int = 0\n",
    "false_positives: int = 0\n",
    "\n",
    "for cnt, message in enumerate(test, 1):\n",
    "    if cnt%100 == 0 or cnt == len(test): print(\"Progress: {}/{}\".format(cnt, len(test)))\n",
    "    \n",
    "    scr: int = nb.predict(message.text)\n",
    "    test_result: bool = True if scr > 0.5 else False\n",
    "\n",
    "    if test_result == True and message.is_spam == True:\n",
    "        true_positives += 1\n",
    "    elif test_result == False and message.is_spam == False:\n",
    "        true_negatives += 1\n",
    "    elif test_result == True and message.is_spam == False:\n",
    "        false_negatives += 1\n",
    "    elif test_result == False and message.is_spam == True:\n",
    "        false_positives += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.345361 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:2f} %'.format((true_positives + true_negatives)*100.0 / len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "    117\t93\n",
      "    44\t522\n"
     ]
    }
   ],
   "source": [
    "print('''Confusion matrix:\n",
    "    {}\\t{}\n",
    "    {}\\t{}'''.format(true_positives, false_positives, false_negatives, true_negatives))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
